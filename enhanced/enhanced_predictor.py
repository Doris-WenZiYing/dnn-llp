import numpy as np
import pandas as pd
import pickle
import joblib
import networkx as nx
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
import seaborn as sns
import os

# ====== é…ç½® ======
MODEL_PATH = "./training_results/topology_aware_dnn_model.keras"   # ç•¶å‰ç›®éŒ„ä¸‹
SCALER_PATH = "./enhanced_scaler.pkl"                              # ç•¶å‰ç›®éŒ„ä¸‹
LABEL_MAP_PATH = "./enhanced_label_mappings.pkl"                   # ç•¶å‰ç›®éŒ„ä¸‹
TOPOLOGY_INFO_PATH = "./topology_info.pkl"                         # ç•¶å‰ç›®éŒ„ä¸‹
RESULTS_DIR = "./prediction_results"                               # ç•¶å‰ç›®éŒ„ä¸‹

os.makedirs(RESULTS_DIR, exist_ok=True)

class TopologyAwarePredictor:
    """æ‹“æ’²æ„ŸçŸ¥RWAé æ¸¬å™¨"""
    
    def __init__(self):
        self.model = None
        self.scaler = None
        self.label_info = None
        self.topology_info = None
        self.feature_dims = None
        
    def load_model_components(self):
        """è¼‰å…¥æ‰€æœ‰æ¨¡å‹çµ„ä»¶"""
        print("ğŸ“¥ è¼‰å…¥æ¨¡å‹çµ„ä»¶...")
        
        # è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
        self.model = load_model(MODEL_PATH)
        print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        
        # è¼‰å…¥ç‰¹å¾µæ¨™æº–åŒ–å™¨
        self.scaler = joblib.load(SCALER_PATH)
        print(f"âœ… æ¨™æº–åŒ–å™¨è¼‰å…¥æˆåŠŸ")
        
        # è¼‰å…¥æ¨™ç±¤æ˜ å°„
        with open(LABEL_MAP_PATH, 'rb') as f:
            self.label_info = pickle.load(f)
        self.feature_dims = self.label_info['feature_dimensions']
        print(f"âœ… æ¨™ç±¤æ˜ å°„è¼‰å…¥æˆåŠŸï¼ŒåŒ…å« {len(self.label_info['label_to_solution'])} å€‹è§£æ±ºæ–¹æ¡ˆ")
        
        # è¼‰å…¥æ‹“æ’²è³‡è¨Š
        with open(TOPOLOGY_INFO_PATH, 'rb') as f:
            self.topology_info = pickle.load(f)
        print(f"âœ… æ‹“æ’²è³‡è¨Šè¼‰å…¥æˆåŠŸï¼ŒåŒ…å« {len(self.topology_info)} ç¨®æ‹“æ’²")
    
    def extract_topology_features(self, G):
        """æå–å–®å€‹æ‹“æ’²çš„ç‰¹å¾µï¼ˆèˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´ï¼‰"""
        n = G.number_of_nodes()
        features = {}
        
        # é„°æ¥çŸ©é™£
        adj_matrix = nx.adjacency_matrix(G).todense()
        features['adjacency_matrix'] = np.array(adj_matrix)
        
        # æœ€çŸ­è·¯å¾‘çŸ©é™£
        try:
            shortest_paths = dict(nx.all_pairs_shortest_path_length(G))
            path_matrix = np.full((n, n), float('inf'))
            for i in range(n):
                for j in range(n):
                    if j in shortest_paths[i]:
                        path_matrix[i][j] = shortest_paths[i][j]
            max_path = np.max(path_matrix[path_matrix != float('inf')])
            path_matrix[path_matrix == float('inf')] = max_path + 1
        except:
            path_matrix = np.full((n, n), n)
        
        features['shortest_path_matrix'] = path_matrix
        
        # ç¯€é»åº¦æ•¸
        in_degrees = [G.in_degree(i) for i in range(n)]
        out_degrees = [G.out_degree(i) for i in range(n)]
        features['node_degrees'] = np.array([in_degrees, out_degrees]).T
        
        # èšé¡ä¿‚æ•¸
        clustering = nx.clustering(G.to_undirected())
        features['clustering_coeffs'] = np.array([clustering[i] for i in range(n)])
        
        # é€£é€šæ€§æŒ‡æ¨™
        features['edge_density'] = nx.density(G)
        features['average_path_length'] = np.mean(path_matrix[path_matrix < n])
        
        return features
    
    def create_enhanced_features(self, tm, topology_name):
        """å»ºç«‹å¢å¼·ç‰¹å¾µå‘é‡"""
        flat_tm = tm.flatten()
        
        if topology_name not in self.topology_info:
            raise ValueError(f"æœªçŸ¥æ‹“æ’²é¡å‹: {topology_name}")
        
        topo_features = self.topology_info[topology_name]['features']
        
        # çµ„åˆæ‰€æœ‰ç‰¹å¾µ
        enhanced_features = np.concatenate([
            flat_tm,
            topo_features['adjacency_matrix'].flatten(),
            topo_features['shortest_path_matrix'].flatten(),
            topo_features['node_degrees'].flatten(),
            topo_features['clustering_coeffs'],
            [topo_features['edge_density']],
            [topo_features['average_path_length']]
        ])
        
        return enhanced_features.reshape(1, -1)
    
    def predict_rwa(self, tm, topology_name):
        """é æ¸¬çµ¦å®šTMå’Œæ‹“æ’²çš„RWAè§£æ±ºæ–¹æ¡ˆ"""
        # å»ºç«‹å¢å¼·ç‰¹å¾µ
        enhanced_features = self.create_enhanced_features(tm, topology_name)
        
        # æ¨™æº–åŒ–
        features_scaled = self.scaler.transform(enhanced_features)
        
        # é æ¸¬
        prediction = self.model.predict(features_scaled, verbose=0)
        predicted_class = np.argmax(prediction)
        confidence = np.max(prediction)
        
        # æŸ¥æ‰¾å°æ‡‰çš„è§£æ±ºæ–¹æ¡ˆ
        solution = None
        if predicted_class in self.label_info['label_to_solution']:
            solution_data = self.label_info['label_to_solution'][predicted_class]
            solution = solution_data['solution']
            predicted_topology = solution_data['topology']
        else:
            predicted_topology = "Unknown"
        
        return {
            'predicted_class': predicted_class,
            'confidence': confidence,
            'solution': solution,
            'predicted_topology': predicted_topology,
            'input_topology': topology_name
        }
    
    def test_all_topologies(self, tm_index=0):
        """æ¸¬è©¦åŒä¸€å€‹TMåœ¨æ‰€æœ‰æ‹“æ’²ä¸‹çš„è¡¨ç¾"""
        print(f"\nğŸ§ª æ¸¬è©¦TM {tm_index} åœ¨æ‰€æœ‰æ‹“æ’²ä¸‹çš„é æ¸¬çµæœ...")
        
        results = {}
        tm_file = f"./tms_8nodes_enhanced/full_mesh/tm_{tm_index:04d}.csv"
        
        if not os.path.exists(tm_file):
            print(f"âŒ TMæ–‡ä»¶ä¸å­˜åœ¨: {tm_file}")
            return None
        
        # è¼‰å…¥åŸºæº–TM
        tm = pd.read_csv(tm_file, header=None).values
        
        for topo_name in self.topology_info.keys():
            try:
                result = self.predict_rwa(tm, topo_name)
                results[topo_name] = result
                
                status = "âœ…" if result['confidence'] > 0.8 else "âš ï¸"
                print(f"  {status} {topo_name.upper()}: "
                      f"Label={result['predicted_class']}, "
                      f"Confidence={result['confidence']:.3f}")
                
            except Exception as e:
                print(f"  âŒ {topo_name.upper()}: é æ¸¬å¤±æ•— - {str(e)}")
                results[topo_name] = None
        
        return results
    
    def detailed_solution_analysis(self, solution, topology_name):
        """è©³ç´°åˆ†æé æ¸¬çš„è§£æ±ºæ–¹æ¡ˆ"""
        if solution is None:
            return "ç„¡æœ‰æ•ˆè§£æ±ºæ–¹æ¡ˆ"
        
        G = self.topology_info[topology_name]['graph']
        analysis = []
        
        analysis.append(f"æ‹“æ’²: {topology_name.upper()}")
        analysis.append(f"è·¯ç”±æ–¹æ¡ˆæ•¸é‡: {len(solution)}")
        analysis.append("\nè©³ç´°è·¯ç”±å®‰æ’:")
        
        for (s, d), (path_id, wavelength) in solution.items():
            try:
                # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›è·¯å¾‘è³‡è¨Šä¾†é¡¯ç¤ºå®Œæ•´è·¯å¾‘
                analysis.append(f"  éœ€æ±‚ {s}â†’{d}: è·¯å¾‘ID={path_id}, æ³¢é•·={wavelength}")
            except:
                analysis.append(f"  éœ€æ±‚ {s}â†’{d}: è·¯å¾‘ID={path_id}, æ³¢é•·={wavelength}")
        
        return "\n".join(analysis)
    
    def batch_evaluation(self, num_samples=50):
        """æ‰¹é‡è©•ä¼°æ¨¡å‹åœ¨æ‰€æœ‰æ‹“æ’²ä¸Šçš„æ€§èƒ½"""
        print(f"\nğŸ“Š æ‰¹é‡è©•ä¼° {num_samples} å€‹æ¨£æœ¬...")
        
        topology_accuracies = {topo: [] for topo in self.topology_info.keys()}
        overall_results = []
        
        for i in range(min(num_samples, 100)):  # é™åˆ¶æœ€å¤§æ¸¬è©¦æ•¸é‡
            try:
                tm_results = self.test_all_topologies(i)
                if tm_results:
                    for topo, result in tm_results.items():
                        if result and result['confidence'] > 0.5:  # åªè€ƒæ…®æœ‰ä¿¡å¿ƒçš„é æ¸¬
                            topology_accuracies[topo].append(result['confidence'])
                    overall_results.append(tm_results)
            except:
                continue
        
        # è¨ˆç®—çµ±è¨ˆçµæœ
        stats = {}
        for topo, confidences in topology_accuracies.items():
            if confidences:
                stats[topo] = {
                    'mean_confidence': np.mean(confidences),
                    'std_confidence': np.std(confidences),
                    'samples': len(confidences),
                    'accuracy_80plus': np.mean(np.array(confidences) >= 0.8)
                }
            else:
                stats[topo] = {
                    'mean_confidence': 0,
                    'std_confidence': 0,
                    'samples': 0,
                    'accuracy_80plus': 0
                }
        
        return stats, overall_results
    
    def visualize_results(self, stats):
        """è¦–è¦ºåŒ–æ‰¹é‡è©•ä¼°çµæœ"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        topologies = list(stats.keys())
        
        # å¹³å‡ä¿¡å¿ƒåº¦
        mean_confs = [stats[topo]['mean_confidence'] for topo in topologies]
        bars1 = ax1.bar(topologies, mean_confs)
        ax1.set_title('Average Prediction Confidence by Topology')
        ax1.set_ylabel('Confidence')
        ax1.set_ylim(0, 1)
        ax1.axhline(y=0.8, color='red', linestyle='--', label='80% Threshold')
        
        for bar, conf in zip(bars1, mean_confs):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{conf:.3f}', ha='center', va='bottom')
        ax1.legend()
        
        # 80%ä»¥ä¸Šæº–ç¢ºç‡æ¯”ä¾‹
        acc_80plus = [stats[topo]['accuracy_80plus'] for topo in topologies]
        bars2 = ax2.bar(topologies, acc_80plus)
        ax2.set_title('Percentage of Predictions with 80%+ Confidence')
        ax2.set_ylabel('Percentage')
        ax2.set_ylim(0, 1)
        
        for bar, acc in zip(bars2, acc_80plus):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{acc:.2%}', ha='center', va='bottom')
        
        # æ¨£æœ¬æ•¸é‡
        samples = [stats[topo]['samples'] for topo in topologies]
        ax3.bar(topologies, samples)
        ax3.set_title('Number of Valid Predictions')
        ax3.set_ylabel('Count')
        
        # ä¿¡å¿ƒåº¦åˆ†ä½ˆç®±å‹åœ–ï¼ˆéœ€è¦åŸå§‹æ•¸æ“šï¼‰
        ax4.text(0.5, 0.5, 'Confidence Distribution\n(Requires raw data)', 
                ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title('Confidence Distribution by Topology')
        
        plt.tight_layout()
        plt.savefig(f'{RESULTS_DIR}/evaluation_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return fig

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    predictor = TopologyAwarePredictor()
    
    try:
        # è¼‰å…¥æ¨¡å‹çµ„ä»¶
        predictor.load_model_components()
        
        # å–®ä¸€TMæ¸¬è©¦
        print("\n" + "="*60)
        print("ğŸ¯ å–®ä¸€TMå¤šæ‹“æ’²æ¸¬è©¦")
        print("="*60)
        
        single_results = predictor.test_all_topologies(tm_index=0)
        
        if single_results:
            print("\nğŸ“‹ è©³ç´°è§£æ±ºæ–¹æ¡ˆåˆ†æ:")
            for topo, result in single_results.items():
                if result and result['solution']:
                    print(f"\n{topo.upper()}:")
                    analysis = predictor.detailed_solution_analysis(
                        result['solution'], topo
                    )
                    print(analysis)
        
        # æ‰¹é‡è©•ä¼°
        print("\n" + "="*60)
        print("ğŸ“Š æ‰¹é‡æ€§èƒ½è©•ä¼°")
        print("="*60)
        
        stats, batch_results = predictor.batch_evaluation(num_samples=20)
        
        print("\nå„æ‹“æ’²æ€§èƒ½çµ±è¨ˆ:")
        for topo, stat in stats.items():
            status = "âœ…" if stat['accuracy_80plus'] >= 0.8 else "âŒ"
            print(f"{status} {topo.upper()}:")
            print(f"    å¹³å‡ä¿¡å¿ƒåº¦: {stat['mean_confidence']:.3f}")
            print(f"    80%+ä¿¡å¿ƒåº¦æ¯”ä¾‹: {stat['accuracy_80plus']:.2%}")
            print(f"    æœ‰æ•ˆæ¨£æœ¬æ•¸: {stat['samples']}")
        
        # è¦–è¦ºåŒ–çµæœ
        predictor.visualize_results(stats)
        
        # å„²å­˜çµæœ
        with open(f'{RESULTS_DIR}/evaluation_stats.pkl', 'wb') as f:
            pickle.dump({'stats': stats, 'batch_results': batch_results}, f)
        
        # æœ€çµ‚è©•ä¼°
        print("\n" + "="*60)
        print("ğŸ† æœ€çµ‚è©•ä¼°çµæœ")
        print("="*60)
        
        all_above_80 = all(stat['accuracy_80plus'] >= 0.8 for stat in stats.values())
        avg_performance = np.mean([stat['accuracy_80plus'] for stat in stats.values()])
        
        print(f"æ•´é«”å¹³å‡æ€§èƒ½: {avg_performance:.2%}")
        
        if all_above_80:
            print("ğŸ‰ æ­å–œï¼æ‰€æœ‰æ‹“æ’²éƒ½é”åˆ°80%ä»¥ä¸Šé«˜ä¿¡å¿ƒåº¦é æ¸¬æ¯”ä¾‹ï¼")
        else:
            print("âš ï¸ éƒ¨åˆ†æ‹“æ’²æœªé”åˆ°80%ç›®æ¨™")
            
        print(f"\nğŸ“ è©³ç´°çµæœå·²å„²å­˜è‡³ {RESULTS_DIR}/")
        
    except FileNotFoundError as e:
        print(f"âŒ æª”æ¡ˆæœªæ‰¾åˆ°: {e}")
        print("è«‹ç¢ºèªå·²åŸ·è¡Œå®Œæ•´çš„è¨“ç·´æµç¨‹")
    except Exception as e:
        print(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()