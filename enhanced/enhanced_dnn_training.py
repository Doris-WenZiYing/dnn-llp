import pandas as pd
import numpy as np
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (Input, Dense, Dropout, BatchNormalization, 
                                   Concatenate, Reshape, MultiHeadAttention, GlobalMaxPooling1D)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import joblib

# ====== ÂèÉÊï∏Ë®≠ÂÆö ======
DATASET_FILE = "./enhanced_dataset_all_topologies.csv"
LABEL_MAP_FILE = "./enhanced_label_mappings.pkl"
MODEL_NAME = "topology_aware_dnn_model.keras"
SCALER_NAME = "./enhanced_scaler.pkl"
RESULTS_DIR = "./training_results"

import os
os.makedirs(RESULTS_DIR, exist_ok=True)

# ====== ËºâÂÖ•Êï∏ÊìöÂíåÊ®ôÁ±§Êò†Â∞Ñ ======
print("üì• ËºâÂÖ•Â¢ûÂº∑Êï∏ÊìöÈõÜ...")
df = pd.read_csv(DATASET_FILE)
with open(LABEL_MAP_FILE, 'rb') as f:
    label_info = pickle.load(f)

X = df.iloc[:, :-1].values
y = df['label'].values

print(f"ÂéüÂßãÊï∏ÊìöÂΩ¢ÁãÄ: X={X.shape}, y={y.shape}")
print(f"È°ûÂà•Êï∏Èáè: {len(np.unique(y))}")
print(f"ÁâπÂæµÁ∂≠Â∫¶: {X.shape[1]} (64Á∂≠TM + 1Á∂≠ÊãìÊí≤)")

# ====== ÈóúÈçµÔºöÊï∏ÊìöÂπ≥Ë°°Á≠ñÁï• ======
label_counts = Counter(y)
print(f"ÂéüÂßãÈ°ûÂà•ÂàÜ‰Ωà: min={min(label_counts.values())}, max={max(label_counts.values())}")

# Êõ¥ÂØ¨È¨ÜÁöÑÈÅéÊøæÊ¢ù‰ª∂ÔºöÊØèÂÄãÈ°ûÂà•Ëá≥Â∞ë3ÂÄãÊ®£Êú¨
min_samples = 3
valid_labels = [label for label, count in label_counts.items() if count >= min_samples]
valid_indices = np.isin(y, valid_labels)

X_filtered = X[valid_indices]
y_filtered = y[valid_indices]

print(f"ÈÅéÊøæÂæåÊï∏ÊìöÂΩ¢ÁãÄ: X={X_filtered.shape}, y={y_filtered.shape}")
print(f"‰øùÁïôÈ°ûÂà•Êï∏Èáè: {len(np.unique(y_filtered))}")

# ====== Á¢∫‰øùÊãìÊí≤Âπ≥Ë°° ======
# Ê™¢Êü•ÊØèÁ®ÆÊãìÊí≤ÁöÑÊ®£Êú¨ÂàÜ‰Ωà
topo_encoding = {'full_mesh': 0, 'ring': 1, 'mesh': 2, 'random': 3}
reverse_topo = {v: k for k, v in topo_encoding.items()}

print("\nüìä ÂêÑÊãìÊí≤Ê®£Êú¨ÂàÜ‰Ωà:")
for topo_code, topo_name in reverse_topo.items():
    topo_samples = np.sum(X_filtered[:, -1] == topo_code)
    print(f"  {topo_name.upper()}: {topo_samples} Ê®£Êú¨")

# ÈáçÊñ∞Á∑®Á¢ºÊ®ôÁ±§
le = LabelEncoder()
y_encoded = le.fit_transform(y_filtered)
y_categorical = to_categorical(y_encoded)
num_classes = len(np.unique(y_encoded))

# Ë®àÁÆóÈ°ûÂà•Ê¨äÈáç‰ª•ËôïÁêÜ‰∏çÂπ≥Ë°°
class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)
class_weight_dict = dict(zip(np.unique(y_encoded), class_weights))

# ÁâπÂæµÊ®ôÊ∫ñÂåñÔºàÂè™Ê®ôÊ∫ñÂåñTMÁâπÂæµÔºå‰øùÊåÅÊãìÊí≤Á∑®Á¢ºÔºâ
scaler = StandardScaler()
X_tm_scaled = scaler.fit_transform(X_filtered[:, :-1])  # Âè™Ê®ôÊ∫ñÂåñÂâç64Á∂≠
X_scaled = np.column_stack([X_tm_scaled, X_filtered[:, -1]])  # ÈáçÊñ∞ÁµÑÂêà
joblib.dump(scaler, SCALER_NAME)

# ====== ‰øùÊåÅÂéüÊúâDNNÊû∂ÊßãÂü∫Á§é‰∏äÁöÑÊúÄÂ∞è‰øÆÊîπ ======
def create_enhanced_model(input_dim, num_classes):
    """Âü∫ÊñºÂéüÊúâÊû∂ÊßãÁöÑÊúÄÂ∞èÂ¢ûÂº∑"""
    
    # Ëº∏ÂÖ•Â±§
    main_input = Input(shape=(input_dim,), name='main_input')
    
    # ÂàÜÈõ¢TMÁâπÂæµÂíåÊãìÊí≤ÁâπÂæµ
    tm_features = main_input[:, :-1]  # Ââç64Á∂≠ÔºöTMÁâπÂæµ
    topo_features = main_input[:, -1:]  # ÊúÄÂæå1Á∂≠ÔºöÊãìÊí≤Á∑®Á¢º
    
    # TMÁâπÂæµËôïÁêÜË∑ØÂæëÔºà‰øùÊåÅÂéüÊúâÈÇèËºØÔºâ
    tm_dense1 = Dense(256, activation='relu', name='tm_dense1')(tm_features)
    tm_dense1 = BatchNormalization()(tm_dense1)
    tm_dense1 = Dropout(0.3)(tm_dense1)
    tm_dense2 = Dense(128, activation='relu', name='tm_dense2')(tm_dense1)
    
    # ÊãìÊí≤ÁâπÂæµËôïÁêÜË∑ØÂæëÔºàÁ∞°ÂñÆËôïÁêÜÔºâ
    topo_dense = Dense(32, activation='relu', name='topo_dense')(topo_features)
    
    # ÁâπÂæµËûçÂêà
    merged = Concatenate()([tm_dense2, topo_dense])
    
    # ‰øùÊåÅÂéüÊúâÁöÑÊ≥®ÊÑèÂäõÊ©üÂà∂
    attention_input = Reshape((1, -1))(merged)
    attention_output = MultiHeadAttention(
        num_heads=4, 
        key_dim=64,
        name='multi_head_attention'
    )(attention_input, attention_input)
    attention_output = GlobalMaxPooling1D()(attention_output)
    
    # ‰øùÊåÅÂéüÊúâÁöÑÊ∑±Â±§Á∂≤Ë∑ØÁµêÊßã
    x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(attention_output)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    
    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    # Ëº∏Âá∫Â±§
    output = Dense(num_classes, activation='softmax', name='output')(x)
    
    model = Model(inputs=main_input, outputs=output)
    return model

# ====== Âª∫Á´ãÊ®°Âûã ======
print("üèóÔ∏è Âª∫Á´ãÊãìÊí≤ÊÑüÁü•DNNÊ®°Âûã...")
model = create_enhanced_model(X_scaled.shape[1], num_classes)

# Á∑®Ë≠ØÊ®°ÂûãÔºà‰øùÊåÅÂéüÊúâË®≠ÁΩÆÔºâ
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# ====== Êô∫ËÉΩÊï∏ÊìöÂàÜÂâ≤ÔºàÁ¢∫‰øùÊãìÊí≤Âπ≥Ë°°Ôºâ======
def topology_aware_split(X, y_encoded, test_size=0.2):
    """Á¢∫‰øùÊØèÁ®ÆÊãìÊí≤Âú®Ë®ìÁ∑¥ÂíåÊ∏¨Ë©¶ÈõÜ‰∏≠ÈÉΩÊúâ‰ª£Ë°®"""
    train_indices, test_indices = [], []
    
    for topo_code in range(4):  # 0,1,2,3
        topo_indices = np.where(X[:, -1] == topo_code)[0]
        if len(topo_indices) > 5:  # Á¢∫‰øùÊúâË∂≥Â§†Ê®£Êú¨ÂàÜÂâ≤
            topo_train, topo_test = train_test_split(
                topo_indices, test_size=test_size, random_state=42
            )
            train_indices.extend(topo_train)
            test_indices.extend(topo_test)
        else:
            # Ê®£Êú¨Â§™Â∞ëÔºåÂÖ®ÈÉ®ÊîæÂÖ•Ë®ìÁ∑¥ÈõÜ
            train_indices.extend(topo_indices)
    
    return train_indices, test_indices

train_idx, test_idx = topology_aware_split(X_scaled, y_encoded)
X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
y_train, y_test = y_categorical[train_idx], y_categorical[test_idx]

print(f"Ë®ìÁ∑¥ÈõÜÂ§ßÂ∞è: {X_train.shape[0]}")
print(f"Ê∏¨Ë©¶ÈõÜÂ§ßÂ∞è: {X_test.shape[0]}")

# Ê™¢Êü•Ê∏¨Ë©¶ÈõÜÊãìÊí≤ÂàÜ‰Ωà
print("\nüìä Ê∏¨Ë©¶ÈõÜÊãìÊí≤ÂàÜ‰Ωà:")
for topo_code, topo_name in reverse_topo.items():
    test_topo_count = np.sum(X_test[:, -1] == topo_code)
    print(f"  {topo_name.upper()}: {test_topo_count} Ê®£Êú¨")

# ====== Ë®ìÁ∑¥Ë®≠ÂÆö ======
callbacks = [
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1),
    ModelCheckpoint(MODEL_NAME, monitor='val_accuracy', save_best_only=True, verbose=1),
    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
]

# ====== Ë®ìÁ∑¥Ê®°Âûã ======
print("üöÄ ÈñãÂßãË®ìÁ∑¥ÊãìÊí≤ÊÑüÁü•DNN...")
history = model.fit(
    X_train, y_train,
    epochs=200,
    batch_size=32,
    validation_split=0.15,
    callbacks=callbacks,
    class_weight=class_weight_dict,  # ‰ΩøÁî®È°ûÂà•Ê¨äÈáç
    verbose=1
)

# ====== Ë©ï‰º∞Ê®°Âûã ======
print("\nüìä Ë©ï‰º∞Ê®°ÂûãÊÄßËÉΩ...")

test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Êï¥È´îÊ∏¨Ë©¶Ê∫ñÁ¢∫Áéá: {test_acc:.4f}")

# ÂêÑÊãìÊí≤Ê∫ñÁ¢∫ÁéáÂàÜÊûê
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = np.argmax(y_test, axis=1)

# ÊåâÊãìÊí≤Ë®àÁÆóÊ∫ñÁ¢∫Áéá
topo_results = {}
for topo_code, topo_name in reverse_topo.items():
    topo_test_mask = X_test[:, -1] == topo_code
    if np.sum(topo_test_mask) > 0:
        topo_acc = np.mean(y_pred_classes[topo_test_mask] == y_test_classes[topo_test_mask])
        topo_results[topo_name] = {
            'accuracy': topo_acc,
            'samples': np.sum(topo_test_mask)
        }
        print(f"{topo_name.upper()} ÊãìÊí≤Ê∫ñÁ¢∫Áéá: {topo_acc:.4f} (Ê®£Êú¨Êï∏: {np.sum(topo_test_mask)})")

# ====== ÁµêÊûúË¶ñË¶∫Âåñ ======
def plot_training_results():
    """Áπ™Ë£ΩË®ìÁ∑¥ÁµêÊûú"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # Ë®ìÁ∑¥Êõ≤Á∑ö
    ax1.plot(history.history['accuracy'], label='Train Accuracy')
    ax1.plot(history.history['val_accuracy'], label='Val Accuracy')
    ax1.axhline(y=0.8, color='red', linestyle='--', label='80% Target')
    ax1.set_title('Model Accuracy')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True)
    
    ax2.plot(history.history['loss'], label='Train Loss')
    ax2.plot(history.history['val_loss'], label='Val Loss')
    ax2.set_title('Model Loss')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True)
    
    # ÂêÑÊãìÊí≤Ê∫ñÁ¢∫Áéá
    if topo_results:
        topos = list(topo_results.keys())
        accuracies = [topo_results[t]['accuracy'] for t in topos]
        colors = ['green' if acc >= 0.8 else 'orange' if acc >= 0.6 else 'red' for acc in accuracies]
        
        bars = ax3.bar(topos, accuracies, color=colors)
        ax3.set_title('Accuracy by Topology')
        ax3.set_ylabel('Accuracy')
        ax3.set_ylim(0, 1)
        
        # Âú®Êü±ÁãÄÂúñ‰∏äÊ®ôË®ªÊï∏ÂÄº
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{acc:.3f}', ha='center', va='bottom')
        
        # Ê∑ªÂä†80%Âü∫Ê∫ñÁ∑ö
        ax3.axhline(y=0.8, color='red', linestyle='--', label='80% Target')
        ax3.legend()
    
    # È†êÊ∏¨‰ø°ÂøÉÂ∫¶ÂàÜ‰Ωà
    confidence_scores = np.max(y_pred, axis=1)
    ax4.hist(confidence_scores, bins=20, alpha=0.7, color='skyblue')
    ax4.axvline(x=0.8, color='red', linestyle='--', alpha=0.7, label='80% Threshold')
    ax4.set_title('Prediction Confidence Distribution')
    ax4.set_xlabel('Confidence Score')
    ax4.set_ylabel('Frequency')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{RESULTS_DIR}/training_results.png', dpi=300, bbox_inches='tight')
    plt.show()

plot_training_results()

# ====== ÂÑ≤Â≠òÊúÄÁµÇÊ®°ÂûãÂíåÁµêÊûú ======
model.save(f'{RESULTS_DIR}/{MODEL_NAME}')

# ÂÑ≤Â≠òË®ìÁ∑¥ÁµêÊûú
results_summary = {
    'overall_test_accuracy': test_acc,
    'topology_results': topo_results,
    'model_params': {
        'input_dim': X_scaled.shape[1],
        'num_classes': num_classes,
        'feature_dimensions': {'tm_size': 64, 'topo_size': 1}
    },
    'training_samples': len(X_train),
    'test_samples': len(X_test)
}

with open(f'{RESULTS_DIR}/results_summary.pkl', 'wb') as f:
    pickle.dump(results_summary, f)

# ====== ÊúÄÁµÇÂ†±Âëä ======
print("\n" + "="*60)
print("üéØ ÊãìÊí≤ÊÑüÁü•DNNË®ìÁ∑¥ÂÆåÊàêÔºÅ")
print("="*60)
print(f"Êï¥È´îÊ∏¨Ë©¶Ê∫ñÁ¢∫Áéá: {test_acc:.4f}")
print("\nÂêÑÊãìÊí≤Ê∫ñÁ¢∫Áéá:")

all_above_80 = True
for topo, result in topo_results.items():
    accuracy = result['accuracy']
    status = "‚úÖ" if accuracy >= 0.8 else "‚ùå"
    print(f"  {status} {topo.upper()}: {accuracy:.4f}")
    if accuracy < 0.8:
        all_above_80 = False

if all_above_80:
    print("\nüéâ ÊÅ≠ÂñúÔºÅÊâÄÊúâÊãìÊí≤ÈÉΩÈÅîÂà∞80%‰ª•‰∏äÊ∫ñÁ¢∫ÁéáÔºÅ")
else:
    print("\n‚ö†Ô∏è ÈÉ®ÂàÜÊãìÊí≤Êú™ÈÅîÂà∞80%ÁõÆÊ®ôÔºå‰ΩÜÊáâË©≤ÊØîÂéü‰æÜÁöÑ42%ÊúâÂ§ßÂπÖÊîπÂñÑ„ÄÇ")

print(f"\nüìÅ ÁµêÊûúÂ∑≤ÂÑ≤Â≠òËá≥ {RESULTS_DIR}/")
print(f"üìÅ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òËá≥ {RESULTS_DIR}/{MODEL_NAME}")
print(f"üìÅ ÁâπÂæµÊ®ôÊ∫ñÂåñÂô®Â∑≤ÂÑ≤Â≠òËá≥ {SCALER_NAME}")

# ====== ÈóúÈçµË®∫Êñ∑‰ø°ÊÅØ ======
print(f"\nüîß Ë®∫Êñ∑‰ø°ÊÅØ:")
print(f"ÁâπÂæµÁ∂≠Â∫¶: {X_scaled.shape[1]} (64Á∂≠TM + 1Á∂≠ÊãìÊí≤)")
print(f"È°ûÂà•Êï∏Èáè: {num_classes}")
print(f"Ê®£Êú¨Êï∏Èáè: Ë®ìÁ∑¥{len(X_train)}, Ê∏¨Ë©¶{len(X_test)}")
high_confidence_preds = np.sum(np.max(y_pred, axis=1) > 0.8)
print(f"È´ò‰ø°ÂøÉÈ†êÊ∏¨(>80%): {high_confidence_preds}/{len(y_pred)} ({high_confidence_preds/len(y_pred)*100:.1f}%)")