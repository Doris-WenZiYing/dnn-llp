import os
import numpy as np
import pandas as pd
import networkx as nx
from pulp import *
import pickle
from collections import Counter
from networkx.algorithms.simple_paths import all_simple_paths

# ====== ä¿æŒåŸæœ‰åƒæ•¸ ======
N = 8
K_PATHS = 3
MAX_WAVELENGTH = 10
INPUT_DIR = "./tms_8nodes_enhanced"
OUTPUT_CSV = "./enhanced_dataset_all_topologies.csv"
LABEL_MAP_FILE = "./enhanced_label_mappings.pkl"
TOPOLOGY_FILE = "./topology_info.pkl"
DEBUG = True
CUTOFF_LEN = 6

# ====== è¼‰å…¥æ‹“æ’²è³‡è¨Š ======
with open(TOPOLOGY_FILE, 'rb') as f:
    topology_info = pickle.load(f)

# ====== ä¿æŒåŸæœ‰çš„è·¯å¾‘ç”Ÿæˆé‚è¼¯ ======
def k_shortest_paths(graph, source, target, k, cutoff=CUTOFF_LEN):
    """å®Œå…¨ç›¸åŒçš„è·¯å¾‘ç”Ÿæˆé‚è¼¯"""
    try:
        all_paths = list(all_simple_paths(graph, source, target, cutoff=cutoff))
        all_paths = sorted(all_paths, key=lambda p: sum(graph[u][v]['length'] for u, v in zip(p, p[1:])))
        return all_paths[:k]
    except:
        return []

def generate_paths_for_topology(G):
    """å®Œå…¨ç›¸åŒçš„è·¯å¾‘ç”Ÿæˆé‚è¼¯"""
    paths = {}
    demand_list = []
    for s in range(N):
        for d in range(N):
            if s != d:
                path_list = k_shortest_paths(G, s, d, K_PATHS)
                paths[(s, d)] = path_list
                if path_list:
                    demand_list.append((s, d))
                elif DEBUG:
                    print(f"âš ï¸ ç„¡æ³•æ‰¾åˆ° {s} â†’ {d} çš„è·¯å¾‘")
    return paths, demand_list

# ====== ä¿æŒåŸæœ‰çš„æ¨™ç±¤æ˜ å°„é‚è¼¯ ======
def get_solution_signature(solution_dict, demand_list):
    """ä¿æŒåŸæœ‰çš„ç°½åç”Ÿæˆé‚è¼¯"""
    parts = []
    for (s, d) in sorted(demand_list):
        if (s, d) in solution_dict:
            p_id, w = solution_dict[(s, d)]
            parts.append(f"{s}_{d}_{p_id}_{w}")
        else:
            parts.append(f"{s}_{d}_None")
    return "|".join(parts)

def get_or_create_label(solution_dict, demand_list, solution_to_label, label_to_solution, next_label_id):
    """ä¿æŒåŸæœ‰çš„æ¨™ç±¤ç”Ÿæˆé‚è¼¯"""
    signature = get_solution_signature(solution_dict, demand_list)
    if signature in solution_to_label:
        return solution_to_label[signature], next_label_id
    else:
        label_id = next_label_id
        solution_to_label[signature] = label_id
        label_to_solution[label_id] = dict(solution_dict)
        next_label_id += 1
        return label_id, next_label_id

# ====== ä¿æŒåŸæœ‰çš„è·¯å¾‘æˆæœ¬è¨ˆç®— ======
def path_cost(path, G):
    """ä¿æŒåŸæœ‰çš„æˆæœ¬è¨ˆç®—"""
    return sum(G[u][v]['length'] for u, v in zip(path, path[1:]))

# ====== æ‹“æ’²ç·¨ç¢¼ï¼ˆå”¯ä¸€æ–°å¢éƒ¨åˆ†ï¼‰======
topo_encoding = {'full_mesh': 0, 'ring': 1, 'mesh': 2, 'random': 3}

# ====== è™•ç†æ¯ç¨®æ‹“æ’² ======
all_X_data, all_Y_data = [], []
global_solution_to_label = {}
global_label_to_solution = {}
global_next_label_id = 0
global_label_counter = Counter()

for topo_name, topo_data in topology_info.items():
    print(f"\nğŸ”„ è™•ç†æ‹“æ’²: {topo_name.upper()}")
    
    G = topo_data['graph']
    topo_code = topo_encoding[topo_name]  # æ‹“æ’²ç·¨ç¢¼
    
    # ä½¿ç”¨åŸæœ‰çš„è·¯å¾‘ç”Ÿæˆ
    paths, demand_list = generate_paths_for_topology(G)
    print(f"ğŸ“¦ {topo_name} æ‹“æ’²å…± {len(demand_list)} å€‹æœ‰æ•ˆ demands")
    
    topo_dir = f"{INPUT_DIR}/{topo_name}"
    if not os.path.exists(topo_dir):
        print(f"âš ï¸ ç›®éŒ„ä¸å­˜åœ¨: {topo_dir}")
        continue
    
    topo_files = sorted([f for f in os.listdir(topo_dir) if f.endswith(".csv")])
    infeasible_count = 0
    
    for idx, file in enumerate(topo_files):
        tm_path = os.path.join(topo_dir, file)
        tm = pd.read_csv(tm_path, header=None).values
        flat_tm = tm.flatten()
        
        # ====== å®Œå…¨ç›¸åŒçš„ILPæ¨¡å‹ ======
        model = LpProblem(f"RWA_ILP_{topo_name}", LpMinimize)
        x = {}
        
        for (s, d), path_list in paths.items():
            for p_id, path in enumerate(path_list):
                for w in range(MAX_WAVELENGTH):
                    x[(s, d, p_id, w)] = LpVariable(f"x_{s}_{d}_{p_id}_{w}", 0, 1, LpBinary)
        
        # ä¿æŒåŸæœ‰çš„ç›®æ¨™å‡½æ•¸
        cost_terms = []
        for (s, d), path_list in paths.items():
            if not path_list:
                continue
            for p_id, path in enumerate(path_list):
                for w in range(MAX_WAVELENGTH):
                    cost = tm[s][d] * path_cost(path, G) * ((w + 1)**2)
                    cost_terms.append(cost * x[(s, d, p_id, w)])
        
        if cost_terms:
            model += lpSum(cost_terms)
        
        # ä¿æŒåŸæœ‰çš„ç´„æŸæ¢ä»¶
        for (s, d), path_list in paths.items():
            if not path_list:
                continue
            if tm[s][d] > 0:
                model += lpSum(x[(s, d, p_id, w)] for p_id in range(len(path_list)) for w in range(MAX_WAVELENGTH)) == 1
            else:
                for p_id in range(len(path_list)):
                    for w in range(MAX_WAVELENGTH):
                        model += x[(s, d, p_id, w)] == 0
        
        # ä¿æŒåŸæœ‰çš„æ³¢é•·è¡çªç´„æŸ
        for u, v in G.edges():
            for w in range(MAX_WAVELENGTH):
                conflict_vars = []
                for (s, d), path_list in paths.items():
                    for p_id, path in enumerate(path_list):
                        if len(path) > 1 and (u, v) in zip(path, path[1:]):
                            conflict_vars.append(x[(s, d, p_id, w)])
                if conflict_vars:
                    model += lpSum(conflict_vars) <= 1
        
        # ====== ä¿æŒåŸæœ‰çš„æ±‚è§£ ======
        model.solve(PULP_CBC_CMD(msg=0))
        
        if LpStatus[model.status] == 'Optimal':
            solution_dict = {}
            for (s, d), path_list in paths.items():
                for p_id, path in enumerate(path_list):
                    for w in range(MAX_WAVELENGTH):
                        if x[(s, d, p_id, w)].varValue == 1:
                            solution_dict[(s, d)] = (p_id, w)
            
            # ä½¿ç”¨åŸæœ‰çš„æ¨™ç±¤ç”Ÿæˆé‚è¼¯
            tm_label, global_next_label_id = get_or_create_label(
                solution_dict, demand_list,
                global_solution_to_label, global_label_to_solution,
                global_next_label_id
            )
            
            # ====== é—œéµä¿®æ”¹ï¼šåªæ·»åŠ æ‹“æ’²ç·¨ç¢¼åˆ°ç‰¹å¾µ ======
            enhanced_features = np.concatenate([
                flat_tm,          # ä¿æŒåŸæœ‰çš„64ç¶­TMç‰¹å¾µ
                [topo_code]       # åªæ·»åŠ 1ç¶­æ‹“æ’²ç·¨ç¢¼ (0,1,2,3)
            ])
            
            all_X_data.append(enhanced_features)
            all_Y_data.append(tm_label)
            global_label_counter[tm_label] += 1
            
            if DEBUG and idx % 100 == 0:
                print(f"âœ… {topo_name}: {file:<20} â†’ Label {tm_label:<3}")
        else:
            infeasible_count += 1
            if DEBUG and infeasible_count <= 5:
                print(f"âŒ {topo_name}: {file:<20} ç„¡æ³•æ±‚è§£")
    
    print(f"ğŸ“Š {topo_name} å®Œæˆï¼Œç„¡è§£æ•¸é‡: {infeasible_count}")

# ====== å„²å­˜çµæœ ======
df = pd.DataFrame(all_X_data)
df['label'] = all_Y_data
df.to_csv(OUTPUT_CSV, index=False)

print(f"\nğŸŒŸ å·²å„²å­˜å¢å¼·è¨“ç·´é›†è‡³ {OUTPUT_CSV}")
print(f"   æ¨£æœ¬æ•¸: {len(all_Y_data)}")
print(f"   ç‰¹å¾µç¶­åº¦: {len(all_X_data[0]) if all_X_data else 0} (åŸ64ç¶­TM + 1ç¶­æ‹“æ’²)")
print(f"   é¡åˆ¥æ•¸: {len(global_label_counter)}")

# å„²å­˜æ¨™ç±¤æ˜ å°„ï¼ˆä¿æŒåŸæœ‰æ ¼å¼ï¼‰
with open(LABEL_MAP_FILE, 'wb') as f:
    pickle.dump({
        'solution_to_label': global_solution_to_label,
        'label_to_solution': global_label_to_solution,
        'topology_info': topology_info,
        'paths': {},  # ç°¡åŒ–å„²å­˜
        'topo_encoding': topo_encoding
    }, f)

print(f"ğŸ§  æ¨™ç±¤æ˜ å°„å·²å„²å­˜è‡³: {LABEL_MAP_FILE}")

# ====== åˆ†ææ•¸æ“šå¹³è¡¡ ======
print("\nğŸ“Š å„æ‹“æ’²æ¨™ç±¤åˆ†ä½ˆï¼š")
topo_label_stats = {}
for label_id, count in global_label_counter.most_common():
    # çµ±è¨ˆæ¯å€‹æ‹“æ’²çš„æ¨™ç±¤æ•¸é‡
    found_topo = "unknown"
    for topo_name in topology_info.keys():
        topo_samples = sum(1 for i, y in enumerate(all_Y_data) if y == label_id and all_X_data[i][-1] == topo_encoding[topo_name])
        if topo_samples > 0:
            found_topo = topo_name
            break
    
    if found_topo not in topo_label_stats:
        topo_label_stats[found_topo] = 0
    topo_label_stats[found_topo] += count

for topo, count in topo_label_stats.items():
    print(f"  {topo}: {count} å€‹è§£æ±ºæ–¹æ¡ˆ")

print(f"\nğŸ¯ æœ€å¸¸è¦‹çš„ 5 å€‹è§£æ±ºæ–¹æ¡ˆï¼š")
for label_id, count in global_label_counter.most_common(5):
    print(f"  Label {label_id:<3}: {count} æ¬¡")

# æª¢æŸ¥æ•¸æ“šå¹³è¡¡
balanced = all(count > len(all_Y_data) * 0.15 for count in topo_label_stats.values())
print(f"\n{'âœ…' if balanced else 'âš ï¸'} æ•¸æ“šå¹³è¡¡æª¢æŸ¥: {'é€šé' if balanced else 'éœ€è¦èª¿æ•´'}")