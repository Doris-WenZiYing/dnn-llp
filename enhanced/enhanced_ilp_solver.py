import os
import numpy as np
import pandas as pd
import networkx as nx
from pulp import *
import pickle
from collections import Counter
from networkx.algorithms.simple_paths import all_simple_paths

# ====== ä¿æŒåŸæœ‰åƒæ•¸ ======
N = 8
K_PATHS = 3
MAX_WAVELENGTH = 10
INPUT_DIR = "./tms_8nodes_enhanced"
OUTPUT_CSV = "./enhanced_dataset_all_topologies.csv"
LABEL_MAP_FILE = "./enhanced_label_mappings.pkl"
TOPOLOGY_FILE = "./topology_info.pkl"
DEBUG = True
CUTOFF_LEN = 6

# ====== è¼‰å…¥æ‹“æ’²è³‡è¨Š ======
with open(TOPOLOGY_FILE, 'rb') as f:
    topology_info = pickle.load(f)

# ====== ä¿æŒåŸæœ‰çš„è·¯å¾‘ç”Ÿæˆé‚è¼¯ ======
def k_shortest_paths(graph, source, target, k, cutoff=CUTOFF_LEN):
    """å®Œå…¨ç›¸åŒçš„è·¯å¾‘ç”Ÿæˆé‚è¼¯"""
    try:
        all_paths = list(all_simple_paths(graph, source, target, cutoff=cutoff))
        all_paths = sorted(all_paths, key=lambda p: sum(graph[u][v]['length'] for u, v in zip(p, p[1:])))
        return all_paths[:k]
    except:
        return []

def generate_paths_for_topology(G):
    """å®Œå…¨ç›¸åŒçš„è·¯å¾‘ç”Ÿæˆé‚è¼¯"""
    paths = {}
    demand_list = []
    for s in range(N):
        for d in range(N):
            if s != d:
                path_list = k_shortest_paths(G, s, d, K_PATHS)
                paths[(s, d)] = path_list
                if path_list:
                    demand_list.append((s, d))
                elif DEBUG:
                    print(f"âš ï¸ ç„¡æ³•æ‰¾åˆ° {s} â†’ {d} çš„è·¯å¾‘")
    return paths, demand_list

# ====== ä¿æŒåŸæœ‰çš„è·¯å¾‘æˆæœ¬è¨ˆç®— ======
def path_cost(path, G):
    """ä¿æŒåŸæœ‰çš„æˆæœ¬è¨ˆç®—"""
    return sum(G[u][v]['length'] for u, v in zip(path, path[1:]))

# ====== æ‹“æ’²ç·¨ç¢¼ï¼ˆå”¯ä¸€æ–°å¢éƒ¨åˆ†ï¼‰======
topo_encoding = {'full_mesh': 0, 'ring': 1, 'mesh': 2, 'random': 3}

# ====== è™•ç†æ¯ç¨®æ‹“æ’² ======
all_X_data, all_Y_data = [], []
global_label_counter = Counter()

for topo_name, topo_data in topology_info.items():
    print(f"\nğŸ”„ è™•ç†æ‹“æ’²: {topo_name.upper()}")
    
    G = topo_data['graph']
    topo_code = topo_encoding[topo_name]  # æ‹“æ’²ç·¨ç¢¼
    
    # ä½¿ç”¨åŸæœ‰çš„è·¯å¾‘ç”Ÿæˆ
    paths, demand_list = generate_paths_for_topology(G)
    print(f"ğŸ“¦ {topo_name} æ‹“æ’²å…± {len(demand_list)} å€‹æœ‰æ•ˆ demands")
    
    topo_dir = f"{INPUT_DIR}/{topo_name}"
    if not os.path.exists(topo_dir):
        print(f"âš ï¸ ç›®éŒ„ä¸å­˜åœ¨: {topo_dir}")
        continue
    
    topo_files = sorted([f for f in os.listdir(topo_dir) if f.endswith(".csv")])
    infeasible_count = 0
    
    for idx, file in enumerate(topo_files):
        tm_path = os.path.join(topo_dir, file)
        tm = pd.read_csv(tm_path, header=None).values
        flat_tm = tm.flatten()
        
        # ====== å®Œå…¨ç›¸åŒçš„ILPæ¨¡å‹ ======
        model = LpProblem(f"RWA_ILP_{topo_name}", LpMinimize)
        x = {}
        
        for (s, d), path_list in paths.items():
            for p_id, path in enumerate(path_list):
                for w in range(MAX_WAVELENGTH):
                    x[(s, d, p_id, w)] = LpVariable(f"x_{s}_{d}_{p_id}_{w}", 0, 1, LpBinary)
        
        # ä¿æŒåŸæœ‰çš„ç›®æ¨™å‡½æ•¸
        cost_terms = []
        for (s, d), path_list in paths.items():
            if not path_list:
                continue
            for p_id, path in enumerate(path_list):
                for w in range(MAX_WAVELENGTH):
                    cost = tm[s][d] * path_cost(path, G) * ((w + 1)**2)
                    cost_terms.append(cost * x[(s, d, p_id, w)])
        
        if cost_terms:
            model += lpSum(cost_terms)
        
        # ä¿æŒåŸæœ‰çš„ç´„æŸæ¢ä»¶
        for (s, d), path_list in paths.items():
            if not path_list:
                continue
            if tm[s][d] > 0:
                model += lpSum(x[(s, d, p_id, w)] for p_id in range(len(path_list)) for w in range(MAX_WAVELENGTH)) == 1
            else:
                for p_id in range(len(path_list)):
                    for w in range(MAX_WAVELENGTH):
                        model += x[(s, d, p_id, w)] == 0
        
        # ä¿æŒåŸæœ‰çš„æ³¢é•·è¡çªç´„æŸ
        for u, v in G.edges():
            for w in range(MAX_WAVELENGTH):
                conflict_vars = []
                for (s, d), path_list in paths.items():
                    for p_id, path in enumerate(path_list):
                        if len(path) > 1 and (u, v) in zip(path, path[1:]):
                            conflict_vars.append(x[(s, d, p_id, w)])
                if conflict_vars:
                    model += lpSum(conflict_vars) <= 1
        
        # ====== ä¿æŒåŸæœ‰çš„æ±‚è§£ ======
        model.solve(PULP_CBC_CMD(msg=0))
        
        # ====== é—œéµä¿®æ”¹ï¼šç°¡åŒ–æ¨™ç±¤ç”Ÿæˆ ======
        if LpStatus[model.status] == 'Optimal':
            # æ‰¾å‡ºä½¿ç”¨çš„æœ€å¤§æ³¢é•·ç´¢å¼•
            max_wavelength_used = -1
            
            # éæ­·æ‰€æœ‰è§£è®Šæ•¸ï¼Œæ‰¾å‡ºè¢«é¸ä¸­çš„æœ€å¤§æ³¢é•·
            for (s, d), path_list in paths.items():
                for p_id, path in enumerate(path_list):
                    for w in range(MAX_WAVELENGTH):
                        if x[(s, d, p_id, w)].varValue == 1:
                            if w > max_wavelength_used:
                                max_wavelength_used = w
            
            # ä½¿ç”¨æœ€å¤§æ³¢é•·ç´¢å¼•ä½œç‚ºç°¡åŒ–æ¨™ç±¤
            tm_label = max_wavelength_used
            
            # ====== ä¿æŒåŸæœ‰çš„ç‰¹å¾µçµ„åˆï¼š64ç¶­TM + 1ç¶­æ‹“æ’² ======
            enhanced_features = np.concatenate([
                flat_tm,          # ä¿æŒåŸæœ‰çš„64ç¶­TMç‰¹å¾µ
                [topo_code]       # åªæ·»åŠ 1ç¶­æ‹“æ’²ç·¨ç¢¼ (0,1,2,3)
            ])
            
            all_X_data.append(enhanced_features)
            all_Y_data.append(tm_label)
            global_label_counter[tm_label] += 1
            
            if DEBUG and idx % 100 == 0:
                print(f"âœ… {topo_name}: {file:<20} â†’ Max Wavelength {tm_label}")
        else:
            infeasible_count += 1
            if DEBUG and infeasible_count <= 5:
                print(f"âŒ {topo_name}: {file:<20} ç„¡æ³•æ±‚è§£")
    
    print(f"ğŸ“Š {topo_name} å®Œæˆï¼Œç„¡è§£æ•¸é‡: {infeasible_count}")

# ====== å„²å­˜çµæœ ======
df = pd.DataFrame(all_X_data)
df['label'] = all_Y_data
df.to_csv(OUTPUT_CSV, index=False)

print(f"\nğŸŒŸ å·²å„²å­˜ç°¡åŒ–è¨“ç·´é›†è‡³ {OUTPUT_CSV}")
print(f"   æ¨£æœ¬æ•¸: {len(all_Y_data)}")
print(f"   ç‰¹å¾µç¶­åº¦: {len(all_X_data[0]) if all_X_data else 0} (64ç¶­TM + 1ç¶­æ‹“æ’²)")
print(f"   é¡åˆ¥æ•¸: {len(global_label_counter)} (ç°¡åŒ–å¾Œ)")

# ====== å„²å­˜ç°¡åŒ–çš„æ¨™ç±¤æ˜ å°„ ======
simplified_label_info = {
    'label_type': 'max_wavelength_index',
    'label_range': f'0 to {max(all_Y_data) if all_Y_data else -1}',
    'topology_encoding': topo_encoding,
    'feature_dimensions': {'tm_size': 64, 'topo_size': 1},
    'total_samples': len(all_Y_data),
    'total_classes': len(global_label_counter)
}

with open(LABEL_MAP_FILE, 'wb') as f:
    pickle.dump(simplified_label_info, f)

print(f"ğŸ§  ç°¡åŒ–æ¨™ç±¤æ˜ å°„å·²å„²å­˜è‡³: {LABEL_MAP_FILE}")

# ====== åˆ†æç°¡åŒ–å¾Œçš„æ•¸æ“šåˆ†ä½ˆ ======
print("\nğŸ“Š ç°¡åŒ–æ¨™ç±¤åˆ†ä½ˆï¼š")
for label_id, count in global_label_counter.most_common():
    percentage = count / len(all_Y_data) * 100
    print(f"  æœ€å¤§æ³¢é•· {label_id}: {count} æ¬¡ ({percentage:.1f}%)")

print(f"\nğŸ“Š å„æ‹“æ’²æ¨£æœ¬åˆ†ä½ˆï¼š")
topo_stats = {name: 0 for name in topo_encoding.keys()}
for i, features in enumerate(all_X_data):
    topo_code = int(features[-1])  # æœ€å¾Œä¸€ç¶­æ˜¯æ‹“æ’²ç·¨ç¢¼
    topo_name = [name for name, code in topo_encoding.items() if code == topo_code][0]
    topo_stats[topo_name] += 1

for topo_name, count in topo_stats.items():
    percentage = count / len(all_Y_data) * 100 if all_Y_data else 0
    print(f"  {topo_name.upper()}: {count} å€‹æ¨£æœ¬ ({percentage:.1f}%)")

# æª¢æŸ¥æ•¸æ“šå¹³è¡¡
min_topo_samples = min(topo_stats.values()) if topo_stats.values() else 0
max_topo_samples = max(topo_stats.values()) if topo_stats.values() else 0
balance_ratio = min_topo_samples / max_topo_samples if max_topo_samples > 0 else 0

print(f"\n{'âœ…' if balance_ratio > 0.7 else 'âš ï¸'} æ‹“æ’²æ•¸æ“šå¹³è¡¡åº¦: {balance_ratio:.2f}")
print(f"âœ… æ¨™ç±¤ç°¡åŒ–æˆåŠŸ: å¾æ•¸åƒé¡é™è‡³ {len(global_label_counter)} é¡")